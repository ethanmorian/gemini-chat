{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching product URLs:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching product URLs: 100%|██████████| 5/5 [00:11<00:00,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n",
    "}\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--disable-software-rasterizer')\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "start_page = 1\n",
    "total_pages = 5  # 7089\n",
    "# total_pages = 5 # 3013\n",
    "product_pages = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def fetch_product_urls(page):\n",
    "    try:\n",
    "        with webdriver.Chrome(options=chrome_options) as driver:\n",
    "            url = f'https://search.shopping.naver.com/search/all?adQuery=%EA%B0%95%EC%95%84%EC%A7%80%20%EC%82%AC%EB%A3%8C&frm=NVSHATC&npayType=2&origQuery=%EA%B0%95%EC%95%84%EC%A7%80%20%EC%82%AC%EB%A3%8C&pagingIndex={page}&pagingSize=40&productSet=checkout&query=%EA%B0%95%EC%95%84%EC%A7%80%20%EC%82%AC%EB%A3%8C&sort=review&timestamp=&viewType=image'\n",
    "            \n",
    "            # url = f'https://search.shopping.naver.com/search/all?adQuery=%EA%B3%A0%EC%96%91%EC%9D%B4%20%EC%82%AC%EB%A3%8C&frm=NVSHATC&npayType=2&origQuery=%EA%B3%A0%EC%96%91%EC%9D%B4%20%EC%82%AC%EB%A3%8C&pagingIndex={page}&pagingSize=40&productSet=checkout&query=%EA%B3%A0%EC%96%91%EC%9D%B4%20%EC%82%AC%EB%A3%8C&sort=review&timestamp=&viewType=image'\n",
    "\n",
    "            driver.get(url)\n",
    "            WebDriverWait(driver, 5).until(lambda driver: driver.execute_script('return document.readyState') == 'complete')\n",
    "\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            WebDriverWait(driver, 5).until(lambda driver: driver.execute_script('return document.readyState') == 'complete')\n",
    "\n",
    "            full_html = driver.page_source\n",
    "\n",
    "            soup = BeautifulSoup(full_html, 'html.parser')\n",
    "\n",
    "            links = soup.find_all('div', class_='thumbnail_thumb_wrap__RbcYO _wrapper')\n",
    "\n",
    "            with lock:\n",
    "                for link in links:\n",
    "                    product_pages.append(link['href'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in thread {threading.current_thread().name} on page {page}: {e}\")\n",
    "        pass\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    list(tqdm(executor.map(fetch_product_urls, range(start_page, total_pages+1)), total=total_pages, desc='Fetching product URLs'))\n",
    "\n",
    "print(len(product_pages))\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images: 100%|██████████| 200/200 [04:40<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "def download_images(product_page):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        driver.get(product_page)\n",
    "        driver.implicitly_wait(5)\n",
    "\n",
    "        driver.find_element(By.CSS_SELECTOR, 'div._3osy73V_eD._1Hc_ju_IXp > button').click()\n",
    "        driver.implicitly_wait(5)\n",
    "\n",
    "        full_html = driver.page_source\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        soup = BeautifulSoup(full_html, 'html.parser')\n",
    "\n",
    "        product_name = soup.select_one('h3._22kNQuEXmb._copyable').text\n",
    "\n",
    "        valid_product_name = ''.join(c for c in os.path.splitext(product_name)[0] if c.isalnum() or c in (' ', '_')).rstrip()\n",
    "        valid_product_name = valid_product_name.replace(' ', '_')\n",
    "        target_directory = os.path.abspath(f'naver/{valid_product_name}')\n",
    "\n",
    "        os.makedirs(target_directory, exist_ok=True)\n",
    "\n",
    "        image_tags = soup.select('.se-main-container img')\n",
    "\n",
    "        for i, img in enumerate(image_tags):\n",
    "            if 'data-src' in img.attrs and img['data-src'].strip():\n",
    "                image_url = img['data-src']\n",
    "\n",
    "                parsed_url = urlparse(image_url)\n",
    "                filename, extension = os.path.splitext(os.path.basename(parsed_url.path))\n",
    "\n",
    "                file_path = os.path.join(target_directory, f'image_{i}{extension}')\n",
    "\n",
    "                response = session.get(image_url)\n",
    "\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    list(tqdm(executor.map(download_images, product_pages), total=len(product_pages), desc='Downloading images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--disable-software-rasterizer')\n",
    "\n",
    "start_page = 1\n",
    "total_pages = 5  # 7089\n",
    "# total_pages = 5 # 3013\n",
    "product_pages = []\n",
    "\n",
    "def fetch_product_urls(page):\n",
    "    try:\n",
    "        with webdriver.Chrome(options=chrome_options) as driver:\n",
    "            url = f'https://search.shopping.naver.com/search/all?adQuery=%EA%B0%95%EC%95%84%EC%A7%80%20%EC%82%AC%EB%A3%8C&frm=NVSHATC&npayType=2&origQuery=%EA%B0%95%EC%95%84%EC%A7%80%20%EC%82%AC%EB%A3%8C&pagingIndex={page}&pagingSize=40&productSet=checkout&query=%EA%B0%95%EC%95%84%EC%A7%80%20%EC%82%AC%EB%A3%8C&sort=review&timestamp=&viewType=image'\n",
    "            \n",
    "            # url = f'https://search.shopping.naver.com/search/all?adQuery=%EA%B3%A0%EC%96%91%EC%9D%B4%20%EC%82%AC%EB%A3%8C&frm=NVSHATC&npayType=2&origQuery=%EA%B3%A0%EC%96%91%EC%9D%B4%20%EC%82%AC%EB%A3%8C&pagingIndex={page}&pagingSize=40&productSet=checkout&query=%EA%B3%A0%EC%96%91%EC%9D%B4%20%EC%82%AC%EB%A3%8C&sort=review&timestamp=&viewType=image'\n",
    "            \n",
    "            driver.get(url)\n",
    "            WebDriverWait(driver, 5).until(lambda driver: driver.execute_script('return document.readyState') == 'complete')\n",
    "\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            WebDriverWait(driver, 5).until(lambda driver: driver.execute_script('return document.readyState') == 'complete')\n",
    "\n",
    "            links = driver.find_elements(By.CSS_SELECTOR, 'div.thumbnail_thumb_wrap__RbcYO._wrapper')\n",
    "\n",
    "            for link in links:\n",
    "                product_pages.append(link.get_attribute('href'))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on page {page}: {e}\")\n",
    "        pass\n",
    "\n",
    "def download_images(product_page):\n",
    "    try:\n",
    "        with webdriver.Chrome(options=chrome_options) as driver:\n",
    "            driver.get(product_page)\n",
    "            driver.implicitly_wait(5)\n",
    "\n",
    "            driver.find_element(By.CSS_SELECTOR, 'div._3osy73V_eD._1Hc_ju_IXp > button').click()\n",
    "            driver.implicitly_wait(5)\n",
    "\n",
    "            product_name = driver.find_element(By.CSS_SELECTOR, 'h3._22kNQuEXmb._copyable').text\n",
    "\n",
    "            valid_product_name = ''.join(c for c in os.path.splitext(product_name)[0] if c.isalnum() or c in (' ', '_')).rstrip()\n",
    "            valid_product_name = valid_product_name.replace(' ', '_')\n",
    "            target_directory = os.path.abspath(f'naver/{valid_product_name}')\n",
    "\n",
    "            os.makedirs(target_directory, exist_ok=True)\n",
    "\n",
    "            image_tags = driver.find_elements(By.CSS_SELECTOR, '.se-main-container img')\n",
    "\n",
    "            for i, img in enumerate(image_tags):\n",
    "                if 'data-src' in img.get_attribute(\"src\") and img.get_attribute(\"src\").strip():\n",
    "                    image_url = img.get_attribute(\"src\")\n",
    "\n",
    "                    parsed_url = urlparse(image_url)\n",
    "                    filename, extension = os.path.splitext(os.path.basename(parsed_url.path))\n",
    "\n",
    "                    file_path = os.path.join(target_directory, f'image_{i}{extension}')\n",
    "\n",
    "                    response = session.get(image_url)\n",
    "\n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "with Pool() as pool:\n",
    "    list(tqdm(pool.imap(fetch_product_urls, range(start_page, total_pages+1)), total=total_pages, desc='Fetching product URLs'))\n",
    "    list(tqdm(pool.imap(download_images, product_pages), total=len(product_pages), desc='Downloading images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
