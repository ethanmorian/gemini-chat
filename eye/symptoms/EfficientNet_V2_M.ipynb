{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16036efa-cff4-43d6-9044-177bddb959ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > requirements.txt\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaaabab0-360e-408b-b5c9-26cca656d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pytz\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Sampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b59d8",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dbd4be2-139a-4089-9ec2-6c64bd457090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathLabelProcessor:\n",
    "    def __init__(self, base_path, folder_name, devices, symptom):\n",
    "        self.base_path = base_path\n",
    "        self.folder_name = folder_name\n",
    "        self.devices = devices\n",
    "        self.symptom = symptom\n",
    "        \n",
    "        self.label_images()\n",
    "      \n",
    "    def find_folders_by_name(self):\n",
    "        matching_folders = []\n",
    "\n",
    "        for root, dirs, files in os.walk(self.base_path):\n",
    "            for dir_name in dirs:\n",
    "                if self.folder_name in dir_name:\n",
    "                    folder_path = os.path.join(root, dir_name)\n",
    "                    matching_folders.append(folder_path)\n",
    "\n",
    "        return matching_folders\n",
    "\n",
    "    def find_image_json_pairs(self, folder_path):\n",
    "        image_paths = []\n",
    "        json_paths = []\n",
    "\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for image_file in filter(lambda x: x.lower().endswith(('jpg', 'png')), files):\n",
    "                image_path = os.path.join(root, image_file)\n",
    "                json_file = f\"{os.path.splitext(image_path)[0]}.json\"\n",
    "                if os.path.isfile(json_file):\n",
    "                    image_paths.append(image_path)\n",
    "                    json_paths.append(json_file)\n",
    "\n",
    "        return image_paths, json_paths\n",
    "\n",
    "    def label_images(self):\n",
    "        self.labeled_image_paths = []\n",
    "\n",
    "        for folder_path in self.find_folders_by_name():\n",
    "            image_paths, json_paths = self.find_image_json_pairs(folder_path)\n",
    "            \n",
    "            for image_path, json_path in zip(image_paths, json_paths):\n",
    "                with open(json_path) as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                if data['images']['meta']['device'] not in self.devices:\n",
    "                    continue\n",
    "                    \n",
    "                label = 0 if data['label']['label_disease_lv_3'] in self.symptom else 1\n",
    "                self.labeled_image_paths.append((image_path, label))\n",
    "        \n",
    "        symptomatic_count = sum(1 for _, label in self.labeled_image_paths if label == 0)\n",
    "        asymptomatic_count = sum(1 for _, label in self.labeled_image_paths if label == 1)\n",
    "        \n",
    "        weight_class_0 = 1.0 / symptomatic_count\n",
    "        weight_class_1 = 1.0 / asymptomatic_count\n",
    "        self.class_weights = torch.tensor([weight_class_0, weight_class_1])\n",
    "\n",
    "        print(f'Total cases: {len(self.labeled_image_paths)}')\n",
    "        print(f'Number of symptomatic cases: {symptomatic_count}, Number of asymptomatic cases: {asymptomatic_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89608a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases: 98646\n",
      "Number of symptomatic cases: 22339, Number of asymptomatic cases: 76307\n",
      "CPU times: user 6.42 s, sys: 1.87 s, total: 8.29 s\n",
      "Wall time: 8.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_path = 'eye/Train'\n",
    "folder_name = '일반'\n",
    "devices = ['스마트폰', '일반카메라']\n",
    "symptom = ['유', '상', '하', '초기', '비성숙', '성숙']\n",
    "\n",
    "processor = PathLabelProcessor(base_path=base_path,\n",
    "                               folder_name=folder_name,\n",
    "                               devices=devices,\n",
    "                               symptom=symptom)\n",
    "\n",
    "data = processor.labeled_image_paths\n",
    "class_weights = processor.class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9ce9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class ImageDataset():\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 transform,\n",
    "                 test_size,\n",
    "                 seed,\n",
    "                 batch_size,\n",
    "                 shuffle,\n",
    "                 num_workers):\n",
    "        dataset = self.make_dataset(data, transform, test_size, seed)\n",
    "        self.dataloader = self.make_dataloader(dataset, batch_size, shuffle, num_workers)\n",
    "        \n",
    "        \n",
    "    def make_dataset(self, data, transform, test_size=None, seed=42):\n",
    "        if test_size:\n",
    "            train_data, val_data = train_test_split(data, \n",
    "                                                    test_size=test_size,\n",
    "                                                    random_state=seed)\n",
    "            dataset_dict = {'train': train_data,\n",
    "                            'val': val_data}\n",
    "        else:\n",
    "            dataset_dict = {'test' : data}\n",
    "\n",
    "        dataset = {k: CustomDataset(v, transform[k])\n",
    "                   for k, v in dataset_dict.items()}\n",
    "        \n",
    "        return dataset\n",
    "        \n",
    "    def make_dataloader(self, dataset, batch_size, shuffle, num_workers):\n",
    "        dataloader = {k: DataLoader(dataset=dataset[k],\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=shuffle,\n",
    "                                    num_workers=num_workers,\n",
    "                                    pin_memory=True)\n",
    "                      for k in dataset.keys()}\n",
    "        \n",
    "        for k, v in dataloader.items():\n",
    "            self.print_class_distribution(k, v)\n",
    "        \n",
    "        return dataloader\n",
    "    \n",
    "    def compute_class_counts(self, data_loader):\n",
    "        counts = torch.zeros(2, dtype=torch.long)\n",
    "        \n",
    "        for _, labels in data_loader:\n",
    "            counts += torch.bincount(labels, minlength=2)\n",
    "        \n",
    "        return counts\n",
    "\n",
    "    def print_class_distribution(self, phase, data_loader):\n",
    "        print(f\"Class Distribution for {phase}:\")\n",
    "        class_counts = self.compute_class_counts(data_loader)\n",
    "        for class_label, count in enumerate(class_counts):\n",
    "            print(f\"  Class {class_label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60547f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution for train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 0: 17868 samples\n",
      "  Class 1: 61048 samples\n",
      "Class Distribution for val:\n",
      "  Class 0: 4471 samples\n",
      "  Class 1: 15259 samples\n",
      "CPU times: user 1min 8s, sys: 1min 3s, total: 2min 12s\n",
      "Wall time: 7min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform = {'train': transforms.Compose([transforms.Resize((480, 480)),\n",
    "                                          transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.RandomVerticalFlip(),\n",
    "                                          transforms.RandomRotation(degrees=10),\n",
    "                                          transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])]),\n",
    "             'val': transforms.Compose([transforms.Resize((480, 480)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])}\n",
    "test_size = 0.2\n",
    "seed = 42\n",
    "batch_size = 128\n",
    "shuffle = True\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "dataloader = ImageDataset(data=data,\n",
    "                          transform=transform,\n",
    "                          test_size=test_size,\n",
    "                          seed=seed,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162d702",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c6fb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, reduction='mean', device='cuda'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha.to(device) if alpha is not None else None\n",
    "        self.reduction = reduction\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none', ignore_index=-100)\n",
    "\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            focal_loss = self.alpha[targets] * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return focal_loss\n",
    "        else:\n",
    "            raise ValueError(\"Invalid reduction option\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc0b2d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 device,\n",
    "                 dataloader,\n",
    "                 criterion,\n",
    "                 optimizer,\n",
    "                 scheduler):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        model_name = model.__class__.__name__\n",
    "        self.dataloader = dataloader\n",
    "        self.criterion = criterion.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.best_f1_score = 0.0\n",
    "        korea = pytz.timezone('Asia/Seoul')\n",
    "        now = datetime.now(korea)\n",
    "        start_time = now.strftime('%Y%m%d-%H%M%S')\n",
    "        self.name = f'{start_time}_{model_name}_v2_m_symptoms.pth'\n",
    "        self.writer = SummaryWriter(log_dir=f'runs/{self.name}')\n",
    "\n",
    "    def calculate_f1_score(self, predicted, labels):\n",
    "        return f1_score(labels, predicted, average='binary')\n",
    "\n",
    "    def calculate_auc_roc(self, predicted, labels):\n",
    "        return roc_auc_score(labels, predicted)\n",
    "\n",
    "    def run_epoch(self, epoch, num_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            self.model.train() if phase == 'train' else self.model.eval()\n",
    "            dataloader = self.dataloader[phase]\n",
    "\n",
    "            total_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            all_predicted = []\n",
    "            all_labels = []\n",
    "\n",
    "            for inputs, labels in tqdm(dataloader, desc=f'{phase.capitalize()} Epoch {epoch + 1}/{num_epochs}', unit='batch'):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "                        self.optimizer.zero_grad()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    all_predicted.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            accuracy = correct / total\n",
    "            self.writer.add_scalar(f'Loss/{phase}', avg_loss, epoch)\n",
    "            self.writer.add_scalar(f'Accuracy/{phase}', accuracy, epoch)\n",
    "\n",
    "            if phase == 'val':\n",
    "                current_f1_score = self.calculate_f1_score(np.array(all_predicted), np.array(all_labels))\n",
    "                current_auc_roc = self.calculate_auc_roc(np.array(all_predicted), np.array(all_labels))\n",
    "\n",
    "                self.writer.add_scalar('F1 Score/valid', current_f1_score, epoch)\n",
    "                self.writer.add_scalar('AUC-ROC/valid', current_auc_roc, epoch)\n",
    "\n",
    "                if current_f1_score > self.best_f1_score:\n",
    "                    self.best_f1_score = current_f1_score\n",
    "                    torch.save(self.model, self.name)\n",
    "\n",
    "        lr_value = self.scheduler.get_lr()[0]\n",
    "        self.writer.add_scalar('LearningRate', lr_value, epoch)\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.run_epoch(epoch, num_epochs)\n",
    "\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e89e25-1427-4de0-bf41-fcb94349cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.efficientnet_v2_m(weights='DEFAULT')\n",
    "for name, param in model.named_parameters():\n",
    "    if \"last_layer\" not in name:\n",
    "        param.requires_grad = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
    "criterion = FocalLoss(gamma=2, alpha=class_weights, reduction='sum')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "trainer = ModelTrainer(model=model,\n",
    "                       device=device,\n",
    "                       dataloader=dataloader.dataloader,\n",
    "                       criterion=criterion,\n",
    "                       optimizer=optimizer,\n",
    "                       scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "397f7da3-9b44-4ecf-8c4a-43dfd0fa71e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/50:  60%|█████▉    | 369/617 [32:57<21:30,  5.20s/batch]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/50:  60%|█████▉    | 370/617 [33:02<22:03,  5.36s/batch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 82\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_epochs):\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 82\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[8], line 41\u001b[0m, in \u001b[0;36mModelTrainer.run_epoch\u001b[0;34m(self, epoch, num_epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphase\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     44\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064496eb",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4dac2b9-a8a2-4577-8972-126d23e2a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTester:\n",
    "    def __init__(self, path, device, dataloader):\n",
    "        self.device = device\n",
    "        self.dataloader = dataloader\n",
    "        self.load_model(path)\n",
    "        self.evaluate()\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model = torch.load(path)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def classify(self):\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        probabilities = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(self.dataloader):\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                predictions.extend(predicted.cpu().numpy())\n",
    "                labels.extend(targets.cpu().numpy())\n",
    "                probabilities.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "        return predictions, labels, probabilities\n",
    "\n",
    "    def calculate_prob_stats(self, probabilities):\n",
    "        probabilities = np.array(probabilities)\n",
    "        min_probs = np.min(probabilities)\n",
    "        max_probs = np.max(probabilities)\n",
    "        std_probs = np.std(probabilities)\n",
    "        mean_probs = np.mean(probabilities)\n",
    "\n",
    "        return min_probs, max_probs, std_probs, mean_probs\n",
    "\n",
    "    def calculate_percentage(self, value):\n",
    "        return f'{value*100:.2f}%'\n",
    "\n",
    "    def evaluate(self):\n",
    "        predictions, labels, probabilities = self.classify()\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        f1 = f1_score(labels, predictions, average='weighted')\n",
    "\n",
    "        min_probs, max_probs, std_probs, mean_probs = self.calculate_prob_stats(probabilities)\n",
    "\n",
    "        print('Evaluation Results:')\n",
    "        print(f'Confusion Matrix:\\n{cm}')\n",
    "        print(f'Accuracy: {self.calculate_percentage(accuracy)}')\n",
    "        print(f'F1 Score: {self.calculate_percentage(f1)}')\n",
    "        print(f'Mean Probability: {self.calculate_percentage(mean_probs)}')\n",
    "        print(f'Max Probability: {self.calculate_percentage(max_probs)}')\n",
    "        print(f'Min Probability: {self.calculate_percentage(min_probs)}')\n",
    "        print(f'Standard Deviation of Probabilities: {std_probs:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62042bbe-8452-4cbb-9cbc-106474e8b741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases: 13808\n",
      "Number of symptomatic cases: 3272, Number of asymptomatic cases: 10536\n",
      "CPU times: user 872 ms, sys: 320 ms, total: 1.19 s\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_path = 'eye/Valid'\n",
    "folder_name = '일반'\n",
    "devices = ['스마트폰', '일반카메라']\n",
    "symptom = ['유', '상', '하', '초기', '비성숙', '성숙']\n",
    "\n",
    "processor = PathLabelProcessor(base_path=base_path,\n",
    "                               folder_name=folder_name,\n",
    "                               devices=devices,\n",
    "                               symptom=symptom)\n",
    "\n",
    "data = processor.labeled_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8a9ece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution for test:\n",
      "  Class 0: 3272 samples\n",
      "  Class 1: 10536 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 863/863 [03:35<00:00,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Confusion Matrix:\n",
      "[[1350 1922]\n",
      " [6200 4336]]\n",
      "Accuracy: 41.18%\n",
      "F1 Score: 45.31%\n",
      "Mean Probability: 50.00%\n",
      "Max Probability: 66.59%\n",
      "Min Probability: 33.41%\n",
      "Standard Deviation of Probabilities: 0.0405\n",
      "CPU times: user 3min 59s, sys: 22.1 s, total: 4min 21s\n",
      "Wall time: 4min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ModelTester at 0x7f3593bebdf0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "transform = {'test': transforms.Compose([transforms.Resize((480, 480)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])}\n",
    "test_size = None\n",
    "seed = 42\n",
    "batch_size = 16\n",
    "shuffle = False\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "dataloader = ImageDataset(data=data,\n",
    "                          transform=transform,\n",
    "                          test_size=test_size,\n",
    "                          seed=seed,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "path = '20231218-100304_EfficientNet_symptoms.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ModelTester(path=path, device=device, dataloader=dataloader.dataloader['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45a5a255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution for test:\n",
      "  Class 0: 3272 samples\n",
      "  Class 1: 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:52<00:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Confusion Matrix:\n",
      "[[1350 1922]\n",
      " [   0    0]]\n",
      "Accuracy: 41.26%\n",
      "F1 Score: 58.42%\n",
      "Mean Probability: 50.00%\n",
      "Max Probability: 66.59%\n",
      "Min Probability: 33.41%\n",
      "Standard Deviation of Probabilities: 0.0394\n",
      "CPU times: user 57.8 s, sys: 6.47 s, total: 1min 4s\n",
      "Wall time: 1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ModelTester at 0x7f34388307c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dataloader = ImageDataset(data=[item for item in data if item[1] == 0],\n",
    "                          transform=transform,\n",
    "                          test_size=test_size,\n",
    "                          seed=seed,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "ModelTester(path=path, device=device, dataloader=dataloader.dataloader['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f3ae528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution for test:\n",
      "  Class 0: 0 samples\n",
      "  Class 1: 10536 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [02:45<00:00,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Confusion Matrix:\n",
      "[[   0    0]\n",
      " [6200 4336]]\n",
      "Accuracy: 41.15%\n",
      "F1 Score: 58.31%\n",
      "Mean Probability: 50.00%\n",
      "Max Probability: 64.65%\n",
      "Min Probability: 35.35%\n",
      "Standard Deviation of Probabilities: 0.0408\n",
      "CPU times: user 3min 2s, sys: 17.7 s, total: 3min 20s\n",
      "Wall time: 3min 6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ModelTester at 0x7f3438831840>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dataloader = ImageDataset(data=[item for item in data if item[1] == 1],\n",
    "                          transform=transform,\n",
    "                          test_size=test_size,\n",
    "                          seed=seed,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "ModelTester(path=path, device=device, dataloader=dataloader.dataloader['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ccdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreModelTester:\n",
    "    def __init__(self, path, device, dataloader):\n",
    "        self.device = device\n",
    "        self.dataloader = dataloader\n",
    "        self.model = models.vgg16_bn(pretrained=True)\n",
    "        self.load_model(path)\n",
    "        self.evaluate()\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model = models.vgg16_bn(pretrained=True)\n",
    "        nr_filters = self.model.classifier[0].in_features\n",
    "        self.model.classifier = nn.Linear(nr_filters, 1)\n",
    "        state_dict = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "        model_dict = self.model.state_dict()\n",
    "        state_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
    "        model_dict.update(state_dict)\n",
    "        self.model.load_state_dict(model_dict)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def classify(self):\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        probabilities = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(self.dataloader):\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                predictions.extend(predicted.cpu().numpy())\n",
    "                labels.extend(targets.cpu().numpy())\n",
    "                probabilities.extend(probs.max(dim=1).values.cpu().numpy())\n",
    "\n",
    "        return predictions, labels, probabilities\n",
    "\n",
    "    def calculate_prob_stats(self, probabilities):\n",
    "        probabilities = np.array(probabilities)\n",
    "        min_probs = np.min(probabilities)\n",
    "        max_probs = np.max(probabilities)\n",
    "        std_probs = np.std(probabilities)\n",
    "        mean_probs = np.mean(probabilities)\n",
    "\n",
    "        return min_probs, max_probs, std_probs, mean_probs\n",
    "\n",
    "    def evaluate(self):\n",
    "        predictions, labels, probabilities = self.classify()\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        f1 = f1_score(labels, predictions, average='weighted')\n",
    "\n",
    "        min_probs, max_probs, std_probs, mean_probs = self.calculate_prob_stats(probabilities)\n",
    "\n",
    "        print(\"Evaluation Results:\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"Mean Probability: {mean_probs:.4f}\")\n",
    "        print(f\"Max Probability: {max_probs:.4f}\")\n",
    "        print(f\"Min Probability: {min_probs:.4f}\")        \n",
    "        print(f\"Standard Deviation of Probabilities: {std_probs:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "transform = {'test': transforms.Compose([transforms.Resize((240, 240)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])}\n",
    "test_size = None\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "shuffle = false\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "dataloader = ImageDataset(data=data,\n",
    "                          transform=transform,\n",
    "                          test_size=test_size,\n",
    "                          seed=seed,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "path = '숙대모델'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ModelTester(path=path, device=device, dataloader=dataloader.dataloader['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataloader = ImageDataset(data=[item for item in data if item[1] == 0],\n",
    "                          transform=transform,\n",
    "                          test_size=test_size,\n",
    "                          seed=seed,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "ModelTester(path=path, device=device, dataloader=dataloader.dataloader['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataloader = ImageDataset(data=[item for item in data if item[1] == 1],\n",
    "                          transform=transform,\n",
    "                          test_size=test_size,\n",
    "                          seed=seed,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "ModelTester(path=path, device=device, dataloader=dataloader.dataloader['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
