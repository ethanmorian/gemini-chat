{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115fb139-e1de-4a95-9217-2d347222f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > requirements.txt\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e42b28-9d0c-449f-a4f7-48dea558eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dabc5882-3298-4417-8902-fc834dc37bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathLabelProcessor:\n",
    "    def __init__(self, \n",
    "                 base_path,\n",
    "                 folder_name,\n",
    "                 devices,\n",
    "                 symptom):\n",
    "        self.base_path = base_path\n",
    "        self.folder_name = folder_name\n",
    "        self.devices = devices\n",
    "        self.symptom = symptom\n",
    "        \n",
    "        self.label_images()\n",
    "      \n",
    "    def find_folders_by_name(self):\n",
    "        matching_folders = []\n",
    "\n",
    "        for root, dirs, files in os.walk(self.base_path):\n",
    "            for dir_name in dirs:\n",
    "                if self.folder_name in dir_name:\n",
    "                    folder_path = os.path.join(root, dir_name)\n",
    "                    matching_folders.append(folder_path)\n",
    "\n",
    "        for folder_path in matching_folders:\n",
    "            print(folder_path)\n",
    "            \n",
    "        return matching_folders\n",
    "\n",
    "    def find_image_json_pairs(self):\n",
    "        image_paths = []\n",
    "        json_paths = []\n",
    "\n",
    "        for folder_path in self.find_folders_by_name():\n",
    "            for root, dirs, files in os.walk(folder_path):\n",
    "                for image_file in filter(lambda x: x.lower().endswith(('jpg', 'png')), files):\n",
    "                    image_path = os.path.join(root, image_file)\n",
    "                    json_file = f\"{os.path.splitext(image_path)[0]}.json\"\n",
    "                    if os.path.isfile(json_file):\n",
    "                        image_paths.append(image_path)\n",
    "                        json_paths.append(json_file)\n",
    "\n",
    "        print(f'Total images: {len(image_paths)}, Total JSON files: {len(json_paths)}')\n",
    "        \n",
    "        return image_paths, json_paths\n",
    "\n",
    "    def label_images(self):\n",
    "        self.labeled_image_paths = []\n",
    "\n",
    "        for image_path, json_path in zip(*self.find_image_json_pairs()):\n",
    "            with open(json_path) as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            if data['images']['meta']['device'] in self.devices:\n",
    "                label = 0 if data['label']['label_disease_lv_3'] in self.symptom else 1\n",
    "                self.labeled_image_paths.append((image_path, label))\n",
    "        \n",
    "        self.symptomatic_count = len(Counter(item for item in self.labeled_image_paths if item[1] == 0))\n",
    "        self.asymptomatic_count = len(Counter(item for item in self.labeled_image_paths if item[1] == 1))\n",
    "\n",
    "        print(f'Total cases: {len(self.labeled_image_paths)}')\n",
    "        print(f'Number of symptomatic cases: {self.symptomatic_count}, Number of asymptomatic cases: {self.asymptomatic_count}')\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, labeled_image_paths, transform):\n",
    "        self.labeled_image_paths = labeled_image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labeled_image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.labeled_image_paths[idx]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        image = self.transform(image)\n",
    "        label = torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class DataLoaderMaker:\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 batch_size,\n",
    "                 train_ratio=0,\n",
    "                 num_workers=None):\n",
    "        self.dataset = dataset\n",
    "        self.train_ratio = train_ratio\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        if train_ratio:\n",
    "            self.split_and_make_dataloader()\n",
    "        else:\n",
    "            self.dataloader = self.make_dataloader(self.dataset)\n",
    "\n",
    "    def make_dataloader(self, dataset, shuffle=False):\n",
    "        dataloader = DataLoader(dataset=dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                shuffle=shuffle,\n",
    "                                num_workers=self.num_workers,\n",
    "                                pin_memory=True)\n",
    "        \n",
    "        self.inspect_data(dataloader)\n",
    "        \n",
    "        return dataloader\n",
    "\n",
    "    def split_and_make_dataloader(self):\n",
    "        train_size = int(len(self.dataset) * self.train_ratio)\n",
    "        test_size = len(self.dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(self.dataset, [train_size, test_size])\n",
    "\n",
    "        self.train_loader = self.make_dataloader(train_dataset, shuffle=True)\n",
    "        self.test_loader = self.make_dataloader(test_dataset, shuffle=True)\n",
    "\n",
    "    def inspect_data(self, dataloader):\n",
    "        print(\"Inspecting Data...\")\n",
    "\n",
    "        class_counts = {}\n",
    "        for _, labels in dataloader:\n",
    "            labels = labels.type(torch.int)\n",
    "            for label in labels.tolist():\n",
    "                label_tuple = tuple(label)\n",
    "                class_counts[label_tuple] = class_counts.get(label_tuple, 0) + 1\n",
    "\n",
    "        print(\"- Class Counts:\")\n",
    "        for class_label, count in class_counts.items():\n",
    "            print(f\"  Class {class_label}: {count} samples\")\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 device,\n",
    "                 train_dataloader,\n",
    "                 valid_dataloader,\n",
    "                 loss_fn,\n",
    "                 optimizer,\n",
    "                 scheduler):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.best_f1_score = 0.0\n",
    "        korea = pytz.timezone('Asia/Seoul')\n",
    "        now = datetime.now(korea)\n",
    "        self.start_time = now.strftime('%Y%m%d-%H%M%S')\n",
    "        self.writer = SummaryWriter(log_dir=f'runs/{self.start_time}')\n",
    "\n",
    "    def calculate_f1_score(self, predicted, labels):\n",
    "        return f1_score(labels, predicted, average='binary')\n",
    "\n",
    "    def calculate_auc_roc(self, predicted, labels):\n",
    "        return roc_auc_score(labels, predicted)\n",
    "\n",
    "    def train_one_epoch(self, epoch, num_epochs):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        for step, (inputs, labels) in enumerate(tqdm(self.train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')):\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "    \n",
    "            outputs = self.model(inputs)\n",
    "    \n",
    "            loss = self.loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            predicted = (probs > 0.5).int()\n",
    "    \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "            loss.backward()\n",
    "    \n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "    \n",
    "        self.scheduler.step()\n",
    "        self.writer.add_scalar('LearningRate', self.scheduler.get_last_lr()[0], epoch)\n",
    "    \n",
    "        avg_loss = total_loss / len(self.train_dataloader)\n",
    "        accuracy = correct / total\n",
    "        self.writer.add_scalar('Loss/train', avg_loss, epoch)        \n",
    "        self.writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
    "\n",
    "    def eval_one_epoch(self, epoch):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_predicted = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.valid_dataloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "        \n",
    "                outputs = self.model(inputs)\n",
    "        \n",
    "                loss = self.loss_fn(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "                probs = torch.sigmoid(outputs)\n",
    "                predicted = (probs > 0.5).int()\n",
    "        \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "                all_predicted.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.valid_dataloader)\n",
    "        accuracy = correct / total\n",
    "        self.writer.add_scalar('Loss/valid', avg_loss, epoch)\n",
    "        self.writer.add_scalar('Accuracy/valid', accuracy, epoch)\n",
    "        \n",
    "        current_f1_score = self.calculate_f1_score(np.array(all_predicted), np.array(all_labels))\n",
    "        current_auc_roc = self.calculate_auc_roc(np.array(all_predicted), np.array(all_labels))\n",
    "        \n",
    "        self.writer.add_scalar('F1 Score/valid', current_f1_score, epoch)\n",
    "        self.writer.add_scalar('AUC-ROC/valid', current_auc_roc, epoch)\n",
    "        \n",
    "        if current_f1_score > self.best_f1_score:\n",
    "            self.best_f1_score = current_f1_score\n",
    "            save_path = f'증상/{self.start_time}'.encode('ascii', 'ignore').decode('ascii')\n",
    "            torch.save(self.model, f'{save_path}.pth')\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train_one_epoch(epoch, num_epochs)\n",
    "            self.eval_one_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eye/Train/고양이/안구/일반\n",
      "eye/Train/개/안구/일반\n",
      "Total images: 199816, Total JSON files: 199816\n",
      "Total cases: 98646\n",
      "Number of symptomatic cases: 22339, Number of asymptomatic cases: 76307\n",
      "CPU times: user 5.77 s, sys: 2.1 s, total: 7.87 s\n",
      "Wall time: 7.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_path = 'eye/Train'\n",
    "folder_name = '일반'\n",
    "devices = ['스마트폰', '일반카메라']\n",
    "symptom = ['유', '상', '하', '초기', '비성숙', '성숙']\n",
    "\n",
    "processor  = PathLabelProcessor(base_path=base_path,\n",
    "                                folder_name=folder_name,\n",
    "                                devices=devices,\n",
    "                                symptom=symptom)\n",
    "\n",
    "labeled_image_paths = processor.labeled_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633a7dc1-1577-44e0-bec0-998cfa515e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 136 µs, sys: 63 µs, total: 199 µs\n",
      "Wall time: 205 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform = transforms.Compose([transforms.Resize((240, 240)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "dataset = ImageDataset(labeled_image_paths=labeled_image_paths,\n",
    "                       transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cbcf6b8-73fb-4a16-965b-04318cbb5cab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting Data...\n",
      "- Class Counts:\n",
      "  Class (1,): 61061 samples\n",
      "  Class (0,): 17855 samples\n",
      "Inspecting Data...\n",
      "- Class Counts:\n",
      "  Class (1,): 15246 samples\n",
      "  Class (0,): 4484 samples\n",
      "CPU times: user 20.4 s, sys: 16.8 s, total: 37.2 s\n",
      "Wall time: 51.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 96\n",
    "num_workers = os.cpu_count()\n",
    "train_ratio = 0.8\n",
    "\n",
    "data_loader = DataLoaderMaker(dataset=dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_workers,\n",
    "                              train_ratio=train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2609bdf4-9610-4840-bfc5-34f85a1dfa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.efficientnet_b1()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, 1)\n",
    "pos_weight = torch.tensor([processor.symptomatic_count / processor.asymptomatic_count], device=device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.01)\n",
    "\n",
    "trainer = ModelTrainer(model=model,\n",
    "                       device=device,\n",
    "                       train_dataloader=data_loader.train_loader,\n",
    "                       valid_dataloader=data_loader.test_loader,\n",
    "                       loss_fn=loss_fn,\n",
    "                       optimizer=optimizer,\n",
    "                       scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b392b9d4-1711-489f-af29-19468eab8b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 823/823 [06:59<00:00,  1.96batch/s]\n",
      "Epoch 2/30: 100%|██████████| 823/823 [06:58<00:00,  1.97batch/s]\n",
      "Epoch 3/30: 100%|██████████| 823/823 [06:58<00:00,  1.96batch/s]\n",
      "Epoch 4/30: 100%|██████████| 823/823 [06:58<00:00,  1.97batch/s]\n",
      "Epoch 5/30:   9%|▊         | 70/823 [00:37<06:21,  1.97batch/s]"
     ]
    }
   ],
   "source": [
    "trainer.train(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1cd4f3-5c00-4577-bae9-ff774b26ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_path = 'eye/Valid'\n",
    "folder_name = '일반'\n",
    "devices = ['스마트폰', '일반카메라']\n",
    "symptom = ['유', '상', '하', '초기', '비성숙', '성숙']\n",
    "\n",
    "processor  = PathLabelProcessor(base_path=base_path,\n",
    "                                folder_name=folder_name,\n",
    "                                devices=devices,\n",
    "                                symptom=symptom)\n",
    "\n",
    "labeled_image_paths = processor.labeled_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b8fe2d-b805-48a5-aeec-0ad909159142",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "transform = transforms.Compose([transforms.Resize((240, 240)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "dataset = ImageDataset(labeled_image_paths=labeled_image_paths,\n",
    "                       transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d508cc-b873-4158-a80b-39a7e5afc8d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_size = 32\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "data_loader = DataLoaderMaker(dataset=dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0902981d-8b7e-4a4c-b707-3d8fa550bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTester:\n",
    "    def __init__(self, path, device, dataloader):\n",
    "        self.device = device\n",
    "        self.dataloader = dataloader\n",
    "        self.load_model(path)\n",
    "        self.evaluate()\n",
    "\n",
    "    def load_model(self, path):\n",
    "        loaded_model = torch.load(path)\n",
    "        self.model.load_state_dict(loaded_model.state_dict())\n",
    "        self.model.to(self.device)\n",
    "        print(\"Loaded Model Information:\")\n",
    "        print(self.model)\n",
    "\n",
    "    def classify(self):\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        probabilities = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(self.dataloader):\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                predictions.extend(predicted.cpu().numpy())\n",
    "                labels.extend(targets.cpu().numpy())\n",
    "                probabilities.extend(probs.max(dim=1).values.cpu().numpy())\n",
    "\n",
    "        return predictions, labels, probabilities\n",
    "\n",
    "    def calculate_prob_stats(self, probabilities):\n",
    "        probabilities = np.array(probabilities)\n",
    "        min_probs = np.min(probabilities)\n",
    "        max_probs = np.max(probabilities)\n",
    "        std_probs = np.std(probabilities)\n",
    "        mean_probs = np.mean(probabilities)\n",
    "\n",
    "        return min_probs, max_probs, std_probs, mean_probs\n",
    "\n",
    "    def evaluate(self):\n",
    "        predictions, labels, probabilities = self.classify()\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        precision = precision_score(labels, predictions)\n",
    "        recall = recall_score(labels, predictions)\n",
    "        f1 = f1_score(labels, predictions)\n",
    "\n",
    "        min_probs, max_probs, std_probs, mean_probs = self.calculate_prob_stats(probabilities)\n",
    "\n",
    "        print(\"Evaluation Results:\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"Min Probability: {min_probs:.4f}\")\n",
    "        print(f\"Max Probability: {max_probs:.4f}\")\n",
    "        print(f\"Standard Deviation of Probabilities: {std_probs:.4f}\")\n",
    "        print(f\"Mean Probability: {mean_probs:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c9e02-67d0-4b7b-848e-b77bd5f68339",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '증상/20231123-033657.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ModelTester(path=path, device=device, dataloader=data_loader.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7857ec-724e-4b1c-a39d-7ed155fa4f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier:\n",
    "    def __init__(self, device, image_path, image_transform, model_weights_path, model):\n",
    "        self.image_path = image_path\n",
    "        self.image_transform = image_transform\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.load_model_weights(model_weights_path)\n",
    "\n",
    "    def preprocess_image(self):\n",
    "        image = Image.open(self.image_path)\n",
    "        input_data = self.image_transform(image).unsqueeze(0)\n",
    "        input_data = input_data.to(self.device)\n",
    "        return input_data\n",
    "\n",
    "    def load_model_weights(self, weights_path):\n",
    "        weights = torch.load(weights_path)\n",
    "        self.model.load_state_dict(weights)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def classify_image(self):\n",
    "        self.model.eval()\n",
    "        input_data = self.preprocess_image()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_data)\n",
    "        \n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "        predicted_prob, predicted_class = torch.max(probabilities, 1)\n",
    "        \n",
    "        class_label = \"Disease\" if predicted_class.item() == 0 else \"Normal\"\n",
    "        class_prob = probabilities[0, predicted_class].item() * 100\n",
    "\n",
    "        print(f\"The image is predicted to be {class_label} with {class_prob:.2f}% probability.\")\n",
    "                \n",
    "        return predicted_class.item(), class_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf2ca04-852d-47e0-8d69-699754938886",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_path = 'kIHjZsKQ6HdQ.jpg'\n",
    "image_transform = transforms.Compose([transforms.Resize((240, 240)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "model_weights_path = 'best_model.pth'\n",
    "model = models.efficientnet_b1()\n",
    "num_classes = 2\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "classifier = ImageClassifier(device=device,\n",
    "                             image_path=image_path,\n",
    "                             image_transform=image_transform,\n",
    "                             model_weights_path=model_weights_path,\n",
    "                             model=model)\n",
    "\n",
    "predicted_class = classifier.classify_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe786e-2ee5-4f65-a943-ebdd99ea6a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from your_dataset_module import CustomDataset\n",
    "from your_trainer_module import ModelTrainer\n",
    "\n",
    "def train_tune(config):\n",
    "    # Define your model and other components\n",
    "    model = models.efficientnet_b1()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(num_ftrs, 1)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.01)\n",
    "    \n",
    "    trainer = ModelTrainer(model=model,\n",
    "                           device=device,\n",
    "                           train_dataloader=train_dataloader,\n",
    "                           valid_dataloader=valid_dataloader,\n",
    "                           loss_fn=loss_fn,\n",
    "                           optimizer=optimizer,\n",
    "                           scheduler=scheduler)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        trainer.train_one_epoch(epoch, num_epochs)\n",
    "        trainer.eval_one_epoch(epoch)\n",
    "\n",
    "    return trainer.best_f1_score\n",
    "\n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"weight_decay\": tune.loguniform(1e-6, 1e-2),\n",
    "    \"step_size\": tune.choice([3, 5, 7]),\n",
    "    \"gamma\": tune.choice([0.01, 0.1, 0.5]),\n",
    "}\n",
    "\n",
    "scheduler = ASHAScheduler(max_t=10, grace_period=1)\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_tune,\n",
    "    config=search_space,\n",
    "    num_samples=10,\n",
    "    scheduler=scheduler,\n",
    "    metric=\"f1_score\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "best_hyperparameters = analysis.get_best_config(metric=\"f1_score\", mode=\"max\")\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95829139-666d-4ef2-bd37-fc62f96d3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    " \n",
    "# ### homomorphic filter는 gray scale image에 대해서 밖에 안 되므로\n",
    "# ### YUV color space로 converting한 뒤 Y에 대해 연산을 진행\n",
    "# img = cv2.imread('file path')\n",
    "# img_YUV = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)    \n",
    "# y = img_YUV[::0]    \n",
    " \n",
    "# rows = y.shape[0]    \n",
    "# cols = y.shape[1]\n",
    " \n",
    "# ### illumination elements와 reflectance elements를 분리하기 위해 log를 취함\n",
    "# imgLog = np.log1p(np.array(y, dtype='float') / 255) # y값을 0~1사이로 조정한 뒤 log(x+1)\n",
    " \n",
    "# ### frequency를 이미지로 나타내면 4분면에 대칭적으로 나타나므로 \n",
    "# ### 4분면 중 하나에 이미지를 대응시키기 위해 row와 column을 2배씩 늘려줌\n",
    "# M = 2*rows + 1\n",
    "# N = 2*cols + 1\n",
    " \n",
    "# ### gaussian mask 생성 sigma = 10\n",
    "# sigma = 10\n",
    "# (X, Y) = np.meshgrid(np.linspace(0, N-1, N), np.linspace(0, M-1, M)) # 0~N-1(and M-1) 까지 1단위로 space를 만듬\n",
    "# Xc = np.ceil(N/2) # 올림 연산\n",
    "# Yc = np.ceil(M/2)\n",
    "# gaussianNumerator = (X - Xc)**2 + (Y - Yc)**2 # 가우시안 분자 생성\n",
    " \n",
    "# ### low pass filter와 high pass filter 생성\n",
    "# LPF = np.exp(-gaussianNumerator / (2*sigma*sigma))\n",
    "# HPF = 1 - LPF\n",
    " \n",
    "# ### LPF랑 HPF를 0이 가운데로 오도록iFFT함. \n",
    "# ### 사실 이 부분이 잘 이해가 안 가는데 plt로 이미지를 띄워보니 shuffling을 수행한 효과가 났음\n",
    "# ### 에너지를 각 귀퉁이로 모아 줌\n",
    "# LPF_shift = np.fft.ifftshift(LPF.copy())\n",
    "# HPF_shift = np.fft.ifftshift(HPF.copy())\n",
    " \n",
    "# ### Log를 씌운 이미지를 FFT해서 LPF와 HPF를 곱해 LF성분과 HF성분을 나눔\n",
    "# img_FFT = np.fft.fft2(imgLog.copy(), (M, N))\n",
    "# img_LF = np.real(np.fft.ifft2(img_FFT.copy() * LPF_shift, (M, N)) # low frequency 성분\n",
    "# img_HF = np.real(np.fft.ifft2(img_FFT.copy() * HPF_shift, (M, N)) # high frequency 성분\n",
    " \n",
    "# ### 각 LF, HF 성분에 scaling factor를 곱해주어 조명값과 반사값을 조절함\n",
    "# gamma1 = 0.3\n",
    "# gamma2 = 1.5\n",
    "# img_adjusting = gamma1*img_LF[0:rows, 0:cols] + gamma2*img_HF[0:rows, 0:cols]\n",
    " \n",
    "# ### 조정된 데이터를 이제 exp 연산을 통해 이미지로 만들어줌\n",
    "# img_exp = np.expm1(img_adjusting) # exp(x) + 1\n",
    "# img_exp = (img_exp - np.min(img_exp)) / (np.max(img_exp) - np.min(img_exp)) # 0~1사이로 정규화\n",
    "# img_out = np.array(255*img_exp, dtype = 'uint8') # 255를 곱해서 intensity값을 만들어줌\n",
    " \n",
    "# ### 마지막으로 YUV에서 Y space를 filtering된 이미지로 교체해주고 RGB space로 converting\n",
    "# img_YUV[:,:,0] = img_out\n",
    "# result = cv2.cvtColor(img_YUV, cv2.COLOR_YUV2BGR)\n",
    "# cv2.imshow('homomorphic', result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
