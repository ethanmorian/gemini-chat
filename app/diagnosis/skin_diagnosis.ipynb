{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "import pytz\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathLabelProcessor:\n",
    "    def __init__(self,\n",
    "                 base_path,\n",
    "                 folder_name,\n",
    "                 lesions):\n",
    "        self.base_path = base_path\n",
    "        self.folder_name = folder_name\n",
    "        self.lesions = lesions\n",
    "        \n",
    "        self.label_images()\n",
    "        \n",
    "    def find_folders_by_name(self):\n",
    "        matching_folders = []\n",
    "\n",
    "        for root, dirs, files in os.walk(self.base_path):\n",
    "            for dir_name in dirs:\n",
    "                if self.folder_name in dir_name:\n",
    "                    folder_path = os.path.join(root, dir_name)\n",
    "                    matching_folders.append(folder_path)\n",
    "\n",
    "        return matching_folders\n",
    "\n",
    "    def find_image_json_pairs(self, folder_path):\n",
    "        image_paths = []\n",
    "        json_paths = []\n",
    "\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for image_file in filter(lambda x: x.lower().endswith(('jpg', 'png')), files):\n",
    "                image_path = os.path.join(root, image_file)\n",
    "                json_file = f\"{os.path.splitext(image_path)[0]}.json\"\n",
    "                if os.path.isfile(json_file):\n",
    "                    image_paths.append(image_path)\n",
    "                    json_paths.append(json_file)\n",
    "\n",
    "        return image_paths, json_paths\n",
    "\n",
    "    def label_images(self):\n",
    "        self.labeled_image_paths = []\n",
    "\n",
    "        for folder_path in self.find_folders_by_name():\n",
    "            image_paths, json_paths = self.find_image_json_pairs(folder_path)\n",
    "            \n",
    "            for image_path, json_path in zip(image_paths, json_paths):\n",
    "                with open(json_path, encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                label = 1 if data['metaData']['lesions'] == self.lesions else 0\n",
    "                self.labeled_image_paths.append((image_path, label))\n",
    "            \n",
    "        asymptomatic_count = sum(1 for _, label in self.labeled_image_paths if label == 0)\n",
    "        symptomatic_count = sum(1 for _, label in self.labeled_image_paths if label == 1)\n",
    "\n",
    "        print(f'Total cases: {len(self.labeled_image_paths)}')\n",
    "        print(f'Number of asymptomatic cases: {asymptomatic_count}, Number of symptomatic cases: {symptomatic_count}')\n",
    "        \n",
    "        weight_class_0 = 1.0 / asymptomatic_count\n",
    "        weight_class_1 = 1.0 / symptomatic_count\n",
    "        self.class_weights = torch.tensor([weight_class_0, weight_class_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases: 433822\n",
      "Number of symptomatic cases: 209523, Number of asymptomatic cases: 224299\n",
      "CPU times: user 1min 13s, sys: 26.6 s, total: 1min 40s\n",
      "Wall time: 9min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_path = 'C:\\Users\\user\\Desktop\\pythonProject\\skin\\01. data\\1.Training'\n",
    "folder_name = '일반'\n",
    "pet_type = 'dog'\n",
    "lesions = 'A1'\n",
    "\n",
    "processor = PathLabelProcessor(base_path=base_path, folder_name=folder_name, pet_type=pet_type lesions=lesions)\n",
    "\n",
    "data = processor.labeled_image_paths\n",
    "class_weights = processor.class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class ImageDataset():\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 transform,\n",
    "                 test_size,\n",
    "                 seed,\n",
    "                 batch_size,\n",
    "                 shuffle,\n",
    "                 num_workers):\n",
    "        dataset = self.make_dataset(data, transform, test_size, seed)\n",
    "        self.dataloader = self.make_dataloader(dataset, batch_size, shuffle, num_workers)\n",
    "        \n",
    "        \n",
    "    def make_dataset(self, data, transform, test_size=None, seed=42):\n",
    "        if test_size:\n",
    "            train_data, val_data = train_test_split(data, \n",
    "                                                    test_size=test_size,\n",
    "                                                    random_state=seed)\n",
    "            dataset_dict = {'train': train_data,\n",
    "                            'val': val_data}\n",
    "        else:\n",
    "            dataset_dict = {'test' : data}\n",
    "\n",
    "        dataset = {k: CustomDataset(v, transform[k])\n",
    "                   for k, v in dataset_dict.items()}\n",
    "        \n",
    "        return dataset\n",
    "        \n",
    "    def make_dataloader(self, dataset, batch_size, shuffle, num_workers):\n",
    "        dataloader = {k: DataLoader(dataset=dataset[k],\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=shuffle,\n",
    "                                    num_workers=num_workers,\n",
    "                                    pin_memory=True)\n",
    "                      for k in dataset.keys()}\n",
    "        \n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution for train:\n",
      "  Class 0: 167708 samples\n",
      "  Class 1: 179349 samples\n",
      "Class Distribution for val:\n",
      "  Class 0: 41815 samples\n",
      "  Class 1: 44950 samples\n",
      "CPU times: user 3min 31s, sys: 3min 44s, total: 7min 16s\n",
      "Wall time: 1h 30min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform = {'train': transforms.Compose([transforms.Resize((240, 240)),\n",
    "                                          transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.RandomVerticalFlip(),\n",
    "                                          transforms.RandomRotation(degrees=10),\n",
    "                                          transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])]),\n",
    "             'val': transforms.Compose([transforms.Resize((240, 240)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])}\n",
    "test_size = 0.15\n",
    "seed = 42\n",
    "batch_size = 256\n",
    "shuffle = True\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "dataloader = ImageDataset(data=data,\n",
    "                          transform=transform,\n",
    "                          test_size=test_size,\n",
    "                          seed=seed,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/50:   0%|          | 0/2712 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/50: 100%|██████████| 2712/2712 [1:14:57<00:00,  1.66s/batch]\n",
      "Val Epoch 1/50: 100%|██████████| 678/678 [20:49<00:00,  1.84s/batch]\n",
      "Train Epoch 2/50: 100%|██████████| 2712/2712 [1:01:36<00:00,  1.36s/batch]\n",
      "Val Epoch 2/50: 100%|██████████| 678/678 [20:51<00:00,  1.85s/batch]\n",
      "Train Epoch 3/50: 100%|██████████| 2712/2712 [1:01:36<00:00,  1.36s/batch]\n",
      "Val Epoch 3/50: 100%|██████████| 678/678 [20:39<00:00,  1.83s/batch]\n",
      "Train Epoch 4/50: 100%|██████████| 2712/2712 [1:01:45<00:00,  1.37s/batch]\n",
      "Val Epoch 4/50: 100%|██████████| 678/678 [20:48<00:00,  1.84s/batch]\n",
      "Train Epoch 5/50: 100%|██████████| 2712/2712 [1:01:26<00:00,  1.36s/batch]\n",
      "Val Epoch 5/50: 100%|██████████| 678/678 [20:47<00:00,  1.84s/batch]\n",
      "Train Epoch 6/50:  66%|██████▌   | 1790/2712 [40:50<20:54,  1.36s/batch]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class CosineAnnealingWarmUpRestarts(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                \n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, reduction='mean', device='cuda'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha.to(device) if alpha is not None else None\n",
    "        self.reduction = reduction\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none', ignore_index=-100)\n",
    "\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            focal_loss = self.alpha[targets] * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return focal_loss\n",
    "        else:\n",
    "            raise ValueError(\"Invalid reduction option\")\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "model = models.efficientnet_b1(weights='DEFAULT')\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
    "for name, param in model.named_parameters():\n",
    "    if \"last_layer\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "start_time = datetime.now(pytz.timezone('Asia/Seoul')).strftime('%Y%m%d-%H%M%S')\n",
    "name = f'{start_time}_{model.__class__.__name__}_b1_symptoms.pt'\n",
    "writer = SummaryWriter(log_dir=f'drharu/ML/diagnosis/runs/{name}')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = FocalLoss(gamma=2, alpha=class_weights, reduction='mean')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0, weight_decay=1e-5)\n",
    "scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=10, T_mult=1, eta_max=1e-3,  T_up=10, gamma=1e-1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for phase in ['train', 'val']:\n",
    "        model.train() if phase == 'train' else model.eval()\n",
    "        dataloader = dataloader[phase]\n",
    "\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_predicted = []\n",
    "        all_labels = []\n",
    "\n",
    "        for inputs, labels in tqdm(dataloader, desc=f'{phase.capitalize()} Epoch {epoch + 1}/{num_epochs}', unit='batch'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                all_predicted.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = correct / total\n",
    "        writer.add_scalar(f'Loss/{phase}', avg_loss, epoch)\n",
    "        writer.add_scalar(f'Accuracy/{phase}', accuracy, epoch)\n",
    "\n",
    "        if phase == 'val':\n",
    "            current_f1_score = f1_score(np.array(all_labels), np.array(all_predicted), average='binary')\n",
    "            current_auc_roc = roc_auc_score(np.array(all_labels), np.array(all_predicted))\n",
    "\n",
    "            writer.add_scalar('F1 Score/valid', current_f1_score, epoch)\n",
    "            writer.add_scalar('AUC-ROC/valid', current_auc_roc, epoch)\n",
    "\n",
    "            if current_f1_score > best_f1_score:\n",
    "                best_f1_score = current_f1_score\n",
    "                torch.save(model.state_dict(), name)\n",
    "\n",
    "    lr_value = scheduler.get_last_lr()[0]\n",
    "    writer.add_scalar('LearningRate', lr_value, epoch)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "writer.close()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_path = 'C:\\Users\\user\\Desktop\\pythonProject\\skin\\01. data\\2.Validation'\n",
    "folder_name = '일반'\n",
    "\n",
    "processor = PathLabelProcessor(base_path=base_path, folder_name=folder_name)\n",
    "\n",
    "data = processor.labeled_image_paths\n",
    "class_weights = processor.class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "transform = {'test': transforms.Compose([transforms.Resize((380, 380)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])}\n",
    "test_size = None\n",
    "seed = 42\n",
    "batch_size = 128\n",
    "shuffle = False\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "dataloader = ImageDataset(data=data,\n",
    "                          transform=transform,\n",
    "                          test_size=test_size,\n",
    "                          seed=seed,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "state_dict_path = '20231229-102149_EfficientNet_b1_개_안검종양.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "model = models.efficientnet_b1()\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "labels = []\n",
    "probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(dataloader.dataloader['test']):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        labels.extend(targets.cpu().numpy())\n",
    "        probabilities.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "predictions, labels, probabilities = classify()\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "f1 = f1_score(labels, predictions, average='weighted')\n",
    "\n",
    "probabilities = np.array(probabilities)\n",
    "min_probs = np.min(probabilities)\n",
    "max_probs = np.max(probabilities)\n",
    "std_probs = np.std(probabilities)\n",
    "mean_probs = np.mean(probabilities)\n",
    "\n",
    "print('Evaluation Results:')\n",
    "print(f'Confusion Matrix:\\n{cm}')\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "print(f'F1 Score: {f1*100:.2f}%')\n",
    "print(f'Mean Probability: {mean_probs*100:.2f}%')\n",
    "print(f'Max Probability: {max_probs*100:.2f}%')\n",
    "print(f'Min Probability: {min_probs*100:.2f}%')\n",
    "print(f'Standard Deviation of Probabilities: {std_probs:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
